import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import random
import re
from typing import List, Dict, Optional


# –∫–æ–Ω—Ñ–∏–≥

# –±–∞–∑–æ–≤—ã–µ urk –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ (–ø—Ä–æ–¥–∞–∂–∞ –∫–≤–∞—Ä—Ç–∏—Ä)
# –ê–ª–º–∞—Ç—ã: /prodazha/kvartiry/almaty/
# –ê—Å—Ç–∞–Ω–∞: /prodazha/kvartiry/astana/
BASE_URLS = {
    '–ê–ª–º–∞—Ç—ã': 'https://krisha.kz/prodazha/kvartiry/almaty/',
    '–ê—Å—Ç–∞–Ω–∞': 'https://krisha.kz/prodazha/kvartiry/astana/'
}

# –°–ø–∏—Å–æ–∫ User-Agent –¥–ª—è —Ä–æ—Ç–∞—Ü–∏–∏ (—Ç–∏–ø–æ –±—Ä–∞—É–∑–µ—Ä)
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
]

# –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞
MAX_PAGES = 5  # –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ (–ø–æ—Ç–æ–º –º–æ–∂–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å)
MIN_DELAY = 2  # –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ (–≤ —Å–µ–∫)
MAX_DELAY = 5  # –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ (–≤ —Å–µ–∫)
REQUEST_TIMEOUT = 30  # —Ç–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞ (–≤ —Å–µ–∫)



# –≤—Å–ø–æ–º–æ–≥ —Ñ—É–Ω–∫—Ü–∏–∏

# —Å–æ–∑–¥–∞—ë–º —Å–µ—Å—Å–∏—é –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è cookies –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
session = requests.Session()


def get_random_headers() -> Dict[str, str]:
    """
    –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª—É—á–∞–π–Ω—ã–µ HTTP-–∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è –∏–º–∏—Ç–∞—Ü–∏–∏ –±—Ä–∞—É–∑–µ—Ä–∞
    –ø–æ–º–æ–≥–∞–µ—Ç –∏–∑–±–µ–∂–∞—Ç—å –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏ —Å–æ —Å—Ç–æ—Ä–æ–Ω—ã —Å–∞–π—Ç–∞
    """
    return {
        'User-Agent': random.choice(USER_AGENTS),
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',
        'Connection': 'keep-alive',
    }


def random_delay():
    """
    –¥–æ–±–∞–≤–ª—è–µ—Ç —Å–ª—É—á–∞–π–Ω—É—é –∑–∞–¥–µ—Ä–∂–∫—É –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
    –≤–∞–∂–Ω–æ –¥–ª—è Anti-detect —Å–∞–π—Ç –Ω–µ –∑–∞–±–ª–æ–∫–∏—Ä—É–µ—Ç –∞–π–ø–∏—à–Ω–∏–∫
    """
    delay = random.uniform(MIN_DELAY, MAX_DELAY)
    print(f"–æ–∂–∏–¥–∞–Ω–∏–µ {delay:.1f} —Å–µ–∫—É–Ω–¥...")
    time.sleep(delay)


def make_request(url: str) -> Optional[BeautifulSoup]:
    """
    –≤—ã–ø–æ–ª–Ω—è–µ—Ç GET-–∑–∞–ø—Ä–æ—Å –∫ —É–∫–∞–∑–∞–Ω–Ω–æ–º—É URL.
    –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–±—ä–µ–∫—Ç BeautifulSoup –∏–ª–∏ None –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏.
    """
    global session
    try:
        headers = get_random_headers()
        
        # –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–µ—Å—Å–∏—é –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è cookies
        response = session.get(url, headers=headers, timeout=REQUEST_TIMEOUT)
        response.raise_for_status()  # –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å –æ—Ç–≤–µ—Ç–∞
        
        # —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—É—é –∫–æ–¥–∏—Ä–æ–≤–∫—É
        response.encoding = 'utf-8'
        
        # –ø–∞—Ä—Å–∏–º HTML —Å –ø–æ–º–æ—â—å—é BeautifulSoup
        soup = BeautifulSoup(response.text, 'html.parser')
        return soup
    
    except requests.exceptions.RequestException as e:
        print(f"–æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ {url}: {e}")
        return None


# —Ñ—É–Ω–∫—Ü–∏—è –ø–∞—Ä—Å–∏–Ω–≥–∞

def get_listing_links(soup: BeautifulSoup) -> List[str]:
    """
    –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏—è —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å–ø–∏—Å–∫–∞.
    
    —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ krisha.kz:
    –°—Å—ã–ª–∫–∏ –Ω–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏—è –∏–º–µ—é—Ç —Ñ–æ—Ä–º–∞—Ç /a/show/XXXXXX
    """
    links = []
    seen = set()  # –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
    
    # –ø–æ–ª—É—á–∞–µ–º –≤–µ—Å—å HTML –∫–∞–∫ —Å—Ç—Ä–æ–∫—É
    html_text = str(soup)
    
    # –∏—â–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏—è —á–µ—Ä–µ–∑ regex
    pattern = r'href=["\']([^"\']*?/a/show/(\d+))[^"\']*["\']'
    matches = re.findall(pattern, html_text)
    
    for href, listing_id in matches:
        # —Ñ–æ—Ä–º–∏—Ä—É–µ–º —á–∏—Å—Ç—ã–π URL
        clean_url = f'https://krisha.kz/a/show/{listing_id}'
        
        # –¥–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏
        if clean_url not in seen:
            seen.add(clean_url)
            links.append(clean_url)
    
    return links


def parse_listing_page(soup: BeautifulSoup, city: str) -> Dict[str, str]:
    """
    –ø–∞—Ä—Å–∏—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ –æ–±—ä—è–≤–ª–µ–Ω–∏—è.
    –∏–∑–≤–ª–µ–∫–∞–µ—Ç: –∑–∞–≥–æ–ª–æ–≤–æ–∫, –æ–ø–∏—Å–∞–Ω–∏–µ, —Ü–µ–Ω—É, –∞–¥—Ä–µ—Å.
    
    –∞—Ä–≥—É–º–µ–Ω—Ç—ã:
        soup: BeautifulSoup –æ–±—ä–µ–∫—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã –æ–±—ä—è–≤–ª–µ–Ω–∏—è
        city: –ù–∞–∑–≤–∞–Ω–∏–µ –≥–æ—Ä–æ–¥–∞ (–ê–ª–º–∞—Ç—ã –∏–ª–∏ –ê—Å—Ç–∞–Ω–∞)
    
    –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        —Å–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ –æ–±—ä—è–≤–ª–µ–Ω–∏—è
    """
    data = {
        'title': '',
        'description': '',
        'price': '',
        'address': '',
        'city': city,
        'url': ''
    }
    
    try:
        # –∑–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ —Ç–µ–≥–µ <title> –∏–ª–∏ <h1>
        title_tag = soup.find('h1')
        if title_tag:
            data['title'] = title_tag.get_text(strip=True)
        else:
            # –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–æ –∏–∑ —Ç–µ–≥–∞ title
            title_tag = soup.find('title')
            if title_tag:
                data['title'] = title_tag.get_text(strip=True).split(' ‚Äî ')[0]
    except Exception as e:
        print(f"    ‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞: {e}")
    
    try:
        # –∏—â–µ–º —Ç–µ–∫—Å—Ç –ø–æ—Å–ª–µ "–û–ø–∏—Å–∞–Ω–∏–µ" –≤ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
        # –æ–ø–∏—Å–∞–Ω–∏–µ –æ–±—ã—á–Ω–æ –≤ —Ç–µ–≥–µ —Å –∫–ª–∞—Å—Å–æ–º –∏–ª–∏ data-–∞—Ç—Ä–∏–±—É—Ç–æ–º
        
        # —Å–ø–æ—Å–æ–± 1: –ò—â–µ–º div —Å —Ç–µ–∫—Å—Ç–æ–º –æ–ø–∏—Å–∞–Ω–∏—è
        desc_text = ""
        
        # –∏—â–µ–º –≤—Å–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –±–ª–æ–∫–∏ –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ–º
        for div in soup.find_all(['div', 'p']):
            text = div.get_text(strip=True)
            # –æ–ø–∏—Å–∞–Ω–∏–µ –æ–±—ã—á–Ω–æ –¥–ª–∏–Ω–Ω–æ–µ –∏ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã–µ —Å–ª–æ–≤–∞
            if len(text) > 100 and ('–∫–≤–∞—Ä—Ç–∏—Ä' in text.lower() or '–∫–æ–º–Ω–∞—Ç' in text.lower() or 
                                     '—Ä–µ–º–æ–Ω—Ç' in text.lower() or '—ç—Ç–∞–∂' in text.lower() or
                                     '—Ä–∞–π–æ–Ω' in text.lower() or '–¥–æ–º' in text.lower()):
                if len(text) > len(desc_text):
                    desc_text = text
        
        # —Å–ø–æ—Å–æ–± 2: –∏—â–µ–º —Ç–µ–∫—Å—Ç –∫–æ—Ç–æ—Ä—ã–π –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å —ç–º–æ–¥–∑–∏ –∏–ª–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö —Å–ª–æ–≤
        if not desc_text:
            all_text = soup.get_text(separator='\n')
            lines = all_text.split('\n')
            for i, line in enumerate(lines):
                line = line.strip()
                if ('–û–ø–∏—Å–∞–Ω–∏–µ' in line and len(line) < 20) or line.startswith('‚ô•'):
                    # —Å–æ–±–∏—Ä–∞–µ–º —Å–ª–µ–¥—É—é—â–∏–µ —Å—Ç—Ä–æ–∫–∏ –∫–∞–∫ –æ–ø–∏—Å–∞–Ω–∏–µ
                    desc_lines = []
                    for j in range(i, min(i+50, len(lines))):
                        if lines[j].strip() and '–¶–µ–Ω–∞ –º2' not in lines[j]:
                            desc_lines.append(lines[j].strip())
                        if '–ü–æ–∂–∞–ª–æ–≤–∞—Ç—å—Å—è' in lines[j] or '–ü–æ–ª–µ–∑–Ω—ã–µ —Å—Ç–∞—Ç—å–∏' in lines[j]:
                            break
                    desc_text = ' '.join(desc_lines)
                    break
        
        data['description'] = desc_text[:5000] if desc_text else ""  # –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
        
    except Exception as e:
        print(f"–æ—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –æ–ø–∏—Å–∞–Ω–∏—è: {e}")
    
    try:
        # —Ü–µ–Ω–∞ –æ–±—ã—á–Ω–æ —Å–æ–¥–µ—Ä–∂–∏—Ç —Å–∏–º–≤–æ–ª —Ç–µ–Ω–≥–µ („Äí) –∏–ª–∏ —Å–ª–æ–≤–æ "–º–ª–Ω"
        price_text = ""
        for tag in soup.find_all(['div', 'span']):
            text = tag.get_text(strip=True)
            if '„Äí' in text and len(text) < 50:
                # –ø—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —ç—Ç–æ —Ü–µ–Ω–∞ (—Å–æ–¥–µ—Ä–∂–∏—Ç —á–∏—Å–ª–æ)
                if any(char.isdigit() for char in text):
                    price_text = text
                    break
        
        if not price_text:
            # –∏—â–µ–º —Ç–µ–∫—Å—Ç –≤–∏–¥–∞ "XX –º–ª–Ω"
            all_text = soup.get_text()
            price_match = re.search(r'(\d[\d\s]*(?:–º–ª–Ω|000\s*000)?\s*„Äí)', all_text)
            if price_match:
                price_text = price_match.group(1).strip()
        
        data['price'] = price_text
        
    except Exception as e:
        print(f"    ‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Ü–µ–Ω—ã: {e}")
    
    try:
        # –∞–¥—Ä–µ—Å –æ–±—ã—á–Ω–æ —Å–æ–¥–µ—Ä–∂–∏—Ç "—Ä-–Ω" (—Ä–∞–π–æ–Ω) –∏–ª–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ —É–ª–∏—Ü—ã
        address_text = ""
        
        # –∏—â–µ–º –≤ —Ç–µ–∫—Å—Ç–µ –ø–æ—Å–ª–µ "–ì–æ—Ä–æ–¥"
        all_text = soup.get_text()
        if '–ì–æ—Ä–æ–¥' in all_text:
            # –ò—â–µ–º —Ç–µ–∫—Å—Ç –ø–æ—Å–ª–µ "–ì–æ—Ä–æ–¥"
            idx = all_text.find('–ì–æ—Ä–æ–¥')
            chunk = all_text[idx:idx+200]
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∞–¥—Ä–µ—Å
            lines = chunk.split('\n')
            for line in lines[1:5]:
                line = line.strip()
                if line and '–ø–æ–∫–∞–∑–∞—Ç—å' not in line.lower():
                    address_text = line
                    break
        
        if not address_text:
            # –ò–∏–µ–º —Ç–µ–∫—Å—Ç —Å –Ω–∞–∑–≤–∞–Ω–∏–µ–º —Ä–∞–π–æ–Ω–∞
            for tag in soup.find_all(['div', 'span']):
                text = tag.get_text(strip=True)
                if '—Ä-–Ω' in text and len(text) < 100:
                    address_text = text
                    break
        
        data['address'] = address_text
        
    except Exception as e:
        print(f"–æ—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –∞–¥—Ä–µ—Å–∞: {e}")
    
    return data


def check_next_page(soup: BeautifulSoup, current_page: int) -> bool:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –µ—Å—Ç—å –ª–∏ —Å–ª–µ–¥—É—é—â–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –≤ –ø–∞–≥–∏–Ω–∞—Ü–∏–∏
    """
    # –∏—â–µ–º –ø–∞–≥–∏–Ω–∞—Ü–∏—é
    pagination = soup.find('nav', class_='paginator')
    if pagination:
        # –ø—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ —Å—Å—ã–ª–∫–∞ –Ω–∞ —Å–ª–µ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—É
        next_link = pagination.find('a', class_='paginator__btn--next')
        if next_link and not next_link.get('disabled'):
            return True
    return False


# –æ—Å–Ω–æ–≤–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª –ø–∞—Ä—Å–µ—Ä–∞

def parse_city(city: str, base_url: str) -> List[Dict[str, str]]:
    """
    –ø–∞—Ä—Å–∏—Ç –≤—Å–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –≥–æ—Ä–æ–¥–∞.
    
    –∞—Ä–≥—É–º–µ–Ω—Ç—ã:
        city: –Ω–∞–∑–≤–∞–Ω–∏–µ –≥–æ—Ä–æ–¥–∞
        base_url: –±–∞–∑–æ–≤—ã–π URL –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
    
    –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –¥–∞–Ω–Ω—ã–º–∏ –æ–±—ä—è–≤–ª–µ–Ω–∏–π
    """
    all_listings = []
    page = 1
    
    print(f"–ø–∞—Ä—Å–∏–Ω–≥ –≥–æ—Ä–æ–¥–∞: {city}")
    
    while page <= MAX_PAGES:
        if page == 1:
            page_url = base_url
        else:
            page_url = f"{base_url}?page={page}"
        
        print(f"—Å—Ç—Ä–∞–Ω–∏—Ü–∞ {page}: {page_url}")
        
        # –ø–æ–ª—É—á–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å–æ —Å–ø–∏—Å–∫–æ–º –æ–±—ä—è–≤–ª–µ–Ω–∏–π
        soup = make_request(page_url)
        if not soup:
            print(f"–Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É {page}")
            break
        
        # –∏–∑–≤–ª–µ–∫–∞–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏—è
        links = get_listing_links(soup)
        print(f"–Ω–∞–π–¥–µ–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ: {len(links)}")
        
        if not links:
            print("–æ–±—ä—è–≤–ª–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –∑–∞–≤–µ—Ä—à–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ –≥–æ—Ä–æ–¥–∞")
            break
        
        # –ø–∞—Ä—Å–∏–º –∫–∞–∂–¥–æ–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ
        for i, link in enumerate(links, 1):
            print(f"    [{i}/{len(links)}] –ü–∞—Ä—Å–∏–Ω–≥: {link}")
            
            # –∑–∞–¥–µ—Ä–∂–∫–∞ –ø–µ—Ä–µ–¥ –∑–∞–ø—Ä–æ—Å–æ–º (Anti-detect)
            random_delay()
            
            # –∑–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –æ–±—ä—è–≤–ª–µ–Ω–∏—è
            listing_soup = make_request(link)
            if not listing_soup:
                print(f"–Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –æ–±—ä—è–≤–ª–µ–Ω–∏–µ")
                continue
            
            # –∞–∞—Ä—Å–∏–º –¥–∞–Ω–Ω—ã–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è
            listing_data = parse_listing_page(listing_soup, city)
            listing_data['url'] = link
            
            # –¥–æ–±–∞–≤–ª—è–µ–º –≤ —Å–ø–∏—Å–æ–∫ –µ—Å–ª–∏ –µ—Å—Ç—å —Ö–æ—Ç—è –±—ã –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏–ª–∏ –æ–ø–∏—Å–∞–Ω–∏–µ
            if listing_data['title'] or listing_data['description']:
                all_listings.append(listing_data)
                print(f"—É—Å–ø–µ—Ö : {listing_data['title'][:50]}...")
            else:
                print(f"–ø—É—Å—Ç–æ–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
        
        # —á–µ–∫–∞–µ–º –µ—Å—Ç—å –ª–∏ —Å–ª–µ–¥—É—é—â–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞
        if not check_next_page(soup, page):
            print(f"\n –ø–æ—Å–ª–µ–¥–Ω—è—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞")
            break
        
        page += 1
        random_delay()  # –∑–∞–¥–µ—Ä–∂–∫–∞ –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ–π
    
    print(f"\n –≥–æ—Ä–æ–¥ {city}: —Å–æ–±—Ä–∞–Ω–æ {len(all_listings)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π")
    return all_listings


def main():
    print("–ø–∞—Ä—Å–µ—Ä")
    print("—Ü–µ–ª—å: —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö")
    print("–≥–æ—Ä–æ–¥–∞: –ê–ª–º–∞—Ç—ã, –ê—Å—Ç–∞–Ω–∞")
    print("–∫–∞—Ç–µ–≥–æ—Ä–∏—è: –ü—Ä–æ–¥–∞–∂–∞ –∫–≤–∞—Ä—Ç–∏—Ä")
    
    all_data = []
    
    # –ø–∞—Ä—Å–∏–º –∫–∞–∂–¥—ã–π –≥–æ—Ä–æ–¥
    for city, base_url in BASE_URLS.items():
        try:
            city_data = parse_city(city, base_url)
            all_data.extend(city_data)
        except Exception as e:
            print(f"\n –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ {city}: {e}")
            continue
    
    # —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    if all_data:
        print("—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö")
        
        # —Å–æ–∑–¥–∞–µ–º DataFrame
        df = pd.DataFrame(all_data)
        
        # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ CSV
        output_file = 'krisha_dataset.csv'
        df.to_csv(output_file, index=False, encoding='utf-8-sig')
        
        print(f"\n–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª: {output_file}")
        print(f"üìä –≤—Å–µ–≥–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π: {len(df)}")
        print(f"\nüìã —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞:")
        print(df.info())
        print(f"\nüìù –ø–µ—Ä–≤—ã–µ 3 –∑–∞–ø–∏—Å–∏:")
        print(df.head(3))
        
        # —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –≥–æ—Ä–æ–¥–∞–º
        print(f"—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –≥–æ—Ä–æ–¥–∞–º:")
        print(df['city'].value_counts())
        
        # —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω–æ—Å—Ç–∏ –æ–ø–∏—Å–∞–Ω–∏–π
        non_empty_desc = df[df['description'].str.len() > 0].shape[0]
        print(f"\n –æ–±—å—è–≤–ª–µ–Ω–∏—è —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º: {non_empty_desc}/{len(df)}")
        
    else:
        print("error")
    

    print("end")



if __name__ == "__main__":
    main()
